{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89eba8fa",
   "metadata": {},
   "source": [
    "### Random forest regressor\n",
    "\n",
    "The theory for building a random forest model for regression problems. This is largely the same as for the random forest classifier, which I assume you are familiar with.\n",
    "\n",
    "As explained in the classifier notebook, the random forest is an ensemble method combining many (modified) decision trees which are each trained on bagged data. Given the (modified) decision tree regressor, the implementation of the random forest regressor is the same as for the classifier, making this a somewhat trivial exercise. We therefore copy most code from the random forest classifier notebook. The only difference is that the final prediction made by the random forest is the average of the predictions of each decision tree. (And we use the root mean square error in the scoring function like for the decision tree regressor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1986fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decision_tree_regressor_modified import DecisionTreeRegressorModified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf7f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bootstrap_samples(X, y, n_estimators, random_state=42):\n",
    "    \"\"\"\n",
    "    Create the bootstrap samples of the given data.\n",
    "    \n",
    "    Parameters:\n",
    "    X (DataFrame): The original dataset.\n",
    "    y (Series): The target values corresponding to the dataset.\n",
    "    n_estimators (int): The number of bootstrap samples to create\n",
    "                        (equal to the number of trees in the forest).\n",
    "    random_state (int): Seed for reproducibility. Defaults to 42.\n",
    "    \n",
    "    Returns:\n",
    "    list: The bootstrap samples of the data.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    sample_size = X.shape[0]\n",
    "    bootstrap_samples = []\n",
    "\n",
    "    for _ in range(n_estimators):\n",
    "        # Generate a random sample with replacement\n",
    "        sample_indices = np.random.choice(sample_size, size=sample_size, replace=True)\n",
    "        X_sample, y_sample = X.iloc[sample_indices], y.iloc[sample_indices]\n",
    "        bootstrap_samples.append( (X_sample, y_sample) )\n",
    "    \n",
    "    return bootstrap_samples\n",
    "\n",
    "def build_forest(bootstrap_samples, n_estimators, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                 verbose=False, threshold=0.0, num_features=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Build the random forest from a decision tree for each bootstrap sample.\n",
    "    \n",
    "    Parameters:\n",
    "    bootstrap_samples (list): The list of bootstrap samples.\n",
    "    n_estimators (int): The number of trees in the forest.\n",
    "    \n",
    "    Returns:\n",
    "    list: The random forest (list of decision trees).\n",
    "    \"\"\"\n",
    "    forest = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        X_sample, y_sample = bootstrap_samples[i]\n",
    "        \n",
    "        # Train a decision tree on the bootstrap sample\n",
    "        tree = DecisionTreeRegressorModified(max_depth=max_depth,\n",
    "                                             min_samples_split=min_samples_split,\n",
    "                                             min_samples_leaf=min_samples_leaf,\n",
    "                                             verbose=verbose,\n",
    "                                             threshold=threshold,\n",
    "                                             num_features=num_features,\n",
    "                                             random_state=random_state)\n",
    "        tree.fit(X_sample, y_sample)\n",
    "        forest.append(tree)\n",
    "    \n",
    "    return forest\n",
    "\n",
    "def predict(X, forest):\n",
    "    \"\"\"\n",
    "    Make predictions using the random forest.\n",
    "    \n",
    "    Parameters:\n",
    "    X (DataFrame): The data to make predictions on.\n",
    "    forest (list): The random forest (list of decision trees).\n",
    "\n",
    "    Returns:\n",
    "    array-like: The predicted labels for the input data.\n",
    "    \"\"\"\n",
    "    predictions = np.array([tree.predict(X) for tree in forest])\n",
    "    # Use the average\n",
    "    forest_predictions = np.mean(predictions, axis=0)\n",
    "    return forest_predictions\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the root mean square error of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): True values.\n",
    "    y_pred (array-like): Predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    float: Root mean square error.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04638ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05  1.05  0.9   0.1  -0.1   0.1 ]\n",
      "Root Mean Square Error: 0.0764\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5, 6],\n",
    "    'feature2': [0, 1, 0, 1, 0, 1],\n",
    "    'target': [1.1, 1.2, 0.9, 0.1, -0.1, 0.0]\n",
    "    })\n",
    "X = test_data[['feature1', 'feature2']]\n",
    "y = test_data['target']\n",
    "n_estimators = 2\n",
    "bootstrap_samples_list = create_bootstrap_samples(X, y, n_estimators)\n",
    "\n",
    "forest = build_forest(bootstrap_samples_list, n_estimators, num_features=2)\n",
    "predictions = predict(X, forest)\n",
    "print(predictions)\n",
    "\n",
    "rmse = score(y, predictions)\n",
    "print(f\"Root Mean Square Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14da96",
   "metadata": {},
   "source": [
    "Now for the more rigorous testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on Iris dataset: 0.3568\n"
     ]
    }
   ],
   "source": [
    "iris_data = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv', header=None)\n",
    "X = iris_data.drop(columns=[0])\n",
    "y = iris_data[0]\n",
    "\n",
    "# Map the labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(X[4].unique())}\n",
    "X[4] = X[4].map(label_mapping)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Do the Forest fit\n",
    "bootstrap_samples_list = create_bootstrap_samples(X_train, y_train, n_estimators=5)\n",
    "forest = build_forest(bootstrap_samples_list, n_estimators=5, max_depth=5)\n",
    "y_pred = predict(X_test, forest)\n",
    "rmse = score(y_test, y_pred)\n",
    "print(f\"Root Mean Square Error on iris dataset: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46896fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error with 1 estimators: 0.4384877452022938\n",
      "Root Mean Square Error with 2 estimators: 0.33544082648778967\n",
      "Root Mean Square Error with 3 estimators: 0.3466004329622881\n",
      "Root Mean Square Error with 4 estimators: 0.3433600806074362\n",
      "Root Mean Square Error with 5 estimators: 0.3568256541469695\n"
     ]
    }
   ],
   "source": [
    "for n_estimators in [1, 2, 3, 4, 5]:\n",
    "    bootstrap_samples_list = create_bootstrap_samples(X_train, y_train, n_estimators=n_estimators)\n",
    "    forest = build_forest(bootstrap_samples_list, n_estimators=n_estimators, max_depth=5)\n",
    "    y_pred = predict(X_test, forest)\n",
    "    accuracy = score(y_test, y_pred)\n",
    "    print(f\"Root Mean Square Error with {n_estimators} estimators: {accuracy}\")\n",
    "# Interestingly, the error is smallest with 2 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn RandomForestRegressor rmse: 0.3418\n"
     ]
    }
   ],
   "source": [
    "# Compare with sklearn's RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sklearn_forest = RandomForestRegressor(n_estimators=5, max_depth=5, random_state=42)\n",
    "sklearn_forest.fit(X_train, y_train)\n",
    "sklearn_predictions = sklearn_forest.predict(X_test)\n",
    "sklearn_rmse = np.sqrt(mean_squared_error(y_test, sklearn_predictions))\n",
    "print(f\"Sklearn RandomForestRegressor rmse: {sklearn_rmse:.4f}\")\n",
    "# Sklearn is slightly better, and also a bit faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb1bd055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on Diabetes dataset: 56.0641\n"
     ]
    }
   ],
   "source": [
    "# Use the diabetes dataset as a regression example\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_data = load_diabetes()\n",
    "X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "y = pd.Series(diabetes_data.target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "bootstrap_samples_list = create_bootstrap_samples(X_train, y_train, n_estimators=10)\n",
    "forest = build_forest(bootstrap_samples_list, n_estimators=10, max_depth=5)\n",
    "y_pred = predict(X_test, forest)\n",
    "rmse = score(y_test, y_pred)\n",
    "print(f\"Root Mean Square Error on Diabetes dataset: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn RandomForestRegressor rmse: 56.6336\n"
     ]
    }
   ],
   "source": [
    "sklearn_forest = RandomForestRegressor(n_estimators=10, max_depth=5, random_state=42)\n",
    "sklearn_forest.fit(X_train, y_train)\n",
    "sklearn_predictions = sklearn_forest.predict(X_test)\n",
    "sklearn_rmse = np.sqrt(mean_squared_error(y_test, sklearn_predictions))\n",
    "print(f\"Sklearn RandomForestRegressor rmse: {sklearn_rmse:.4f}\")\n",
    "# In this case sklearn is slightly worse, but much faster given the larger number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5629d18",
   "metadata": {},
   "source": [
    "Just like for the decision tree test, the predictions we make on the diabetes data are quite bad, but so are those from sklearn, so a random forest does not work well on this data either. Here we also see that it takes our forest about ten seconds to run while sklearn is still instantaneous, so our implementation is much slower.\n",
    "\n",
    "Still, in the test cases it is again clear that our implementation generates good results, comparable to sklearn. It is however fairly slow, just like the classifier, so that this might also not work well for larger datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
